{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPeLG6SEIh6J",
        "outputId": "c8080d79-d307-48ec-a488-ba8965284320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama_Quant_Int4_Triton'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 39 (delta 11), reused 38 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (39/39), 7.24 KiB | 3.62 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vlonekika/Llama_Quant_Int4_Triton.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes torch datasets"
      ],
      "metadata": {
        "id": "cANedaUeIqm_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/Llama_Quant_Int4_Triton\")"
      ],
      "metadata": {
        "id": "S1NPQCDLJS8W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "from research_tools.quant import quantize_int4\n",
        "from research_tools.matmul import matmul_int4\n",
        "from research_tools.utils import quantize_model, model_memory, perplexity, compute_speed\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W7-RH2sGIsQ5",
        "outputId": "bcda235f-b038-4cc4-c4f3-eb979fea4f29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comparing Pytorch quantize vs triton quantize"
      ],
      "metadata": {
        "id": "H_XXCN7EJwtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch quantize\n",
        "def quantize_int4_torch_fast(w):\n",
        "    w = w.contiguous()\n",
        "    n_rows, n_cols = w.shape\n",
        "\n",
        "    min_val = w.min(dim=1, keepdim=True).values\n",
        "    max_val = w.max(dim=1, keepdim=True).values\n",
        "    scale = ((max_val - min_val) / 15.0).clamp(min=1e-6)\n",
        "    zero_point = ((-min_val / scale).round()).clamp(0, 15)\n",
        "\n",
        "    scales = scale.squeeze(1).to(torch.float16)\n",
        "    zeros = zero_point.squeeze(1).to(torch.float16)\n",
        "\n",
        "    q = ((w / scale + zero_point).round()).clamp(0, 15).to(torch.int32)\n",
        "\n",
        "    q_padded = torch.zeros((n_rows, (n_cols + 7) // 8 * 8), dtype=torch.int32, device=w.device)\n",
        "    q_padded[:, :n_cols] = q\n",
        "    shifts = torch.arange(8, device=w.device, dtype=torch.int32) * 4\n",
        "    q_packed = (q_padded.view(n_rows, -1, 8) << shifts).sum(dim=2)\n",
        "\n",
        "    return q_packed, scales, zeros\n",
        "\n",
        "def matmul_int4_torch_fast(x, w_packed, w_scales, w_zeros):\n",
        "    M, K = x.shape\n",
        "    N = w_packed.shape[0]\n",
        "\n",
        "    w_rows = torch.zeros((N, K), device=x.device, dtype=torch.float32)\n",
        "    for i in range(8):\n",
        "        mask = i < (K // 8 * 8)\n",
        "        vals = (w_packed[:, :(K // 8)] >> (i * 4)) & 0xF\n",
        "        w_rows[:, i::8] = (vals.float() - w_zeros[:, None]) * w_scales[:, None]\n",
        "\n",
        "    output = (x.float() @ w_rows.T).to(torch.bfloat16)\n",
        "    return output\n",
        "\n",
        "#Benchmark quantize\n",
        "def benchmark_quantize(w):\n",
        "    results = []\n",
        "\n",
        "    block_sizes = [32, 64, 128, 256, 512]\n",
        "    for BLOCK_SIZE in block_sizes:\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        quantize_int4(w, BLOCK_SIZE=BLOCK_SIZE)\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = (time.time() - start) * 1000\n",
        "        results.append({\"method\": \"triton\", \"BLOCK_SIZE\": BLOCK_SIZE, \"time_ms\": elapsed})\n",
        "\n",
        "    #PyTorch\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    quantize_int4_torch_fast(w)\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = (time.time() - start) * 1000\n",
        "    results.append({\"method\": \"torch\", \"BLOCK_SIZE\": None, \"time_ms\": elapsed})\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df, df.loc[df['time_ms'].idxmin()]\n",
        "\n",
        "#Benchmark matmul\n",
        "def benchmark_matmul(x, w_packed, w_scales, w_zeros):\n",
        "    results = []\n",
        "\n",
        "    configs = [\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 32, \"BLOCK_K\": 16},\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 64, \"BLOCK_K\": 16},\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 64, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 64, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128},\n",
        "        {\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128},\n",
        "    ]\n",
        "\n",
        "    #Triton\n",
        "    for cfg in configs:\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        matmul_int4(\n",
        "            x, w_packed, w_scales, w_zeros,\n",
        "            BLOCK_M=cfg[\"BLOCK_M\"],\n",
        "            BLOCK_N=cfg[\"BLOCK_N\"],\n",
        "            BLOCK_K=cfg[\"BLOCK_K\"]\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = (time.time() - start) * 1000\n",
        "        results.append({**cfg, \"method\": \"triton\", \"time_ms\": elapsed})\n",
        "\n",
        "    #PyTorch\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    matmul_int4_torch_fast(x, w_packed, w_scales, w_zeros)\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = (time.time() - start) * 1000\n",
        "    results.append({\"BLOCK_M\": None, \"BLOCK_N\": None, \"BLOCK_K\": None, \"method\": \"torch\", \"time_ms\": elapsed})\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df, df.loc[df['time_ms'].idxmin()]\n",
        "\n",
        "device = \"cuda\"\n",
        "hidden_size = 1024\n",
        "batch_size = 512\n",
        "w = torch.randn(hidden_size, hidden_size, device=device)\n",
        "x = torch.randn(batch_size, hidden_size, device=device)\n",
        "packed, scales, zeros = quantize_int4(w)\n",
        "\n",
        "df_quantize, best_quantize = benchmark_quantize(w)\n",
        "df_matmul, best_matmul = benchmark_matmul(x, packed, scales, zeros)\n",
        "\n",
        "print(\"Quantize timings:\")\n",
        "print(df_quantize)\n",
        "print(\"Best quantize config:\")\n",
        "print(best_quantize)\n",
        "\n",
        "print(\"\\nMatmul timings:\")\n",
        "print(df_matmul)\n",
        "print(\"Best matmul config:\")\n",
        "print(best_matmul)\n",
        "\n",
        "df_quantize.to_excel(\"df_quantize.xlsx\", index=False)\n",
        "df_matmul.to_excel(\"df_matmul.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmAFQ-OmJJ24",
        "outputId": "79e7908e-c95b-48cb-8c06-db0e43d6ae02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantize timings:\n",
            "   method  BLOCK_SIZE     time_ms\n",
            "0  triton        32.0    3.177881\n",
            "1  triton        64.0    0.679255\n",
            "2  triton       128.0    2.689838\n",
            "3  triton       256.0    2.497435\n",
            "4  triton       512.0    2.451181\n",
            "5   torch         NaN  177.739859\n",
            "Best quantize config:\n",
            "method          triton\n",
            "BLOCK_SIZE        64.0\n",
            "time_ms       0.679255\n",
            "Name: 1, dtype: object\n",
            "\n",
            "Matmul timings:\n",
            "    BLOCK_M  BLOCK_N  BLOCK_K  method    time_ms\n",
            "0      32.0     32.0     16.0  triton  20.141602\n",
            "1      32.0     64.0     16.0  triton   7.694244\n",
            "2      32.0     64.0     32.0  triton   7.466555\n",
            "3      64.0     64.0     32.0  triton   6.075144\n",
            "4      64.0    128.0     32.0  triton   6.243229\n",
            "5      64.0    128.0     64.0  triton   5.468130\n",
            "6     128.0     64.0     64.0  triton   5.099058\n",
            "7     128.0    128.0     64.0  triton   4.697323\n",
            "8     128.0    128.0    128.0  triton   4.979134\n",
            "9     256.0    128.0    128.0  triton  17.190456\n",
            "10      NaN      NaN      NaN   torch  66.637039\n",
            "Best matmul config:\n",
            "BLOCK_M       128.0\n",
            "BLOCK_N       128.0\n",
            "BLOCK_K        64.0\n",
            "method       triton\n",
            "time_ms    4.697323\n",
            "Name: 7, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (X16@W4^T) vs (X16@W16^T)"
      ],
      "metadata": {
        "id": "8fhSVw3lKlg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_triton_vs_vanilla(batch_sizes, hidden_size=2048, n_iters=10):\n",
        "\n",
        "    block_configs = [\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 32, \"BLOCK_K\": 16},\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 64, \"BLOCK_K\": 16},\n",
        "        {\"BLOCK_M\": 32, \"BLOCK_N\": 64, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 64, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32},\n",
        "        {\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 64},\n",
        "        {\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128},\n",
        "        {\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128},\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for batch in batch_sizes:\n",
        "\n",
        "        x = torch.randn(batch, hidden_size, device=\"cuda\", dtype=torch.bfloat16)\n",
        "        w = torch.randn(hidden_size, hidden_size, device=\"cuda\")\n",
        "\n",
        "        w_packed, w_scales, w_zeros = quantize_int4(w)\n",
        "\n",
        "        for cfg in block_configs:\n",
        "            BLOCK_M, BLOCK_N, BLOCK_K = cfg[\"BLOCK_M\"], cfg[\"BLOCK_N\"], cfg[\"BLOCK_K\"]\n",
        "\n",
        "            matmul_int4(x, w_packed, w_scales, w_zeros, BLOCK_M, BLOCK_N, BLOCK_K)\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            t0 = time.time()\n",
        "            for _ in range(n_iters):\n",
        "                matmul_int4(x, w_packed, w_scales, w_zeros, BLOCK_M, BLOCK_N, BLOCK_K)\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed = (time.time() - t0) * 1000 / n_iters\n",
        "\n",
        "            results.append({\n",
        "                \"batch\": batch,\n",
        "                \"method\": \"Triton X@W4^T\",\n",
        "                \"BLOCK_M\": BLOCK_M,\n",
        "                \"BLOCK_N\": BLOCK_N,\n",
        "                \"BLOCK_K\": BLOCK_K,\n",
        "                \"time_ms\": elapsed\n",
        "            })\n",
        "\n",
        "        W16_T = w.t().contiguous()\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "        for _ in range(n_iters):\n",
        "            out16 = (x.float() @ W16_T.float()).to(torch.bfloat16)\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed16 = (time.time() - t1) * 1000 / n_iters\n",
        "\n",
        "        results.append({\n",
        "            \"batch\": batch,\n",
        "            \"method\": \"Pytorch X16@W16^T\",\n",
        "            \"BLOCK_M\": None,\n",
        "            \"BLOCK_N\": None,\n",
        "            \"BLOCK_K\": None,\n",
        "            \"time_ms\": elapsed16\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "token_cnt = [128, 512, 2048]\n",
        "hidden_size = 2048\n",
        "df = benchmark_triton_vs_vanilla(token_cnt, hidden_size=hidden_size, n_iters=5)\n",
        "\n",
        "print(\"Results\")\n",
        "print(df)\n",
        "\n",
        "df.to_excel(\"matmul_quant_vs_vanila.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ynr_PSKN5b",
        "outputId": "559982dd-8e10-472c-c317-f81072d437c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results\n",
            "    batch             method  BLOCK_M  BLOCK_N  BLOCK_K     time_ms\n",
            "0     128      Triton X@W4^T     32.0     32.0     16.0    7.605314\n",
            "1     128      Triton X@W4^T     32.0     64.0     16.0    5.153370\n",
            "2     128      Triton X@W4^T     32.0     64.0     32.0    4.768324\n",
            "3     128      Triton X@W4^T     64.0     64.0     32.0    3.366280\n",
            "4     128      Triton X@W4^T     64.0    128.0     32.0    3.164387\n",
            "5     128      Triton X@W4^T     64.0    128.0     64.0    3.157234\n",
            "6     128      Triton X@W4^T    128.0     64.0     64.0    2.455187\n",
            "7     128      Triton X@W4^T    128.0    128.0     64.0    4.667568\n",
            "8     128      Triton X@W4^T    128.0    128.0    128.0    4.669571\n",
            "9     128      Triton X@W4^T    256.0    128.0    128.0   12.510014\n",
            "10    128  Pytorch X16@W16^T      NaN      NaN      NaN    0.297260\n",
            "11    512      Triton X@W4^T     32.0     32.0     16.0   12.401915\n",
            "12    512      Triton X@W4^T     32.0     64.0     16.0    7.595062\n",
            "13    512      Triton X@W4^T     32.0     64.0     32.0    7.347870\n",
            "14    512      Triton X@W4^T     64.0     64.0     32.0    5.434227\n",
            "15    512      Triton X@W4^T     64.0    128.0     32.0    4.852867\n",
            "16    512      Triton X@W4^T     64.0    128.0     64.0    4.733515\n",
            "17    512      Triton X@W4^T    128.0     64.0     64.0    3.690577\n",
            "18    512      Triton X@W4^T    128.0    128.0     64.0    4.807138\n",
            "19    512      Triton X@W4^T    128.0    128.0    128.0    4.572725\n",
            "20    512      Triton X@W4^T    256.0    128.0    128.0   40.560150\n",
            "21    512  Pytorch X16@W16^T      NaN      NaN      NaN    0.798798\n",
            "22   2048      Triton X@W4^T     32.0     32.0     16.0   47.181511\n",
            "23   2048      Triton X@W4^T     32.0     64.0     16.0   32.546663\n",
            "24   2048      Triton X@W4^T     32.0     64.0     32.0   31.720972\n",
            "25   2048      Triton X@W4^T     64.0     64.0     32.0   22.814751\n",
            "26   2048      Triton X@W4^T     64.0    128.0     32.0   16.993189\n",
            "27   2048      Triton X@W4^T     64.0    128.0     64.0   17.041540\n",
            "28   2048      Triton X@W4^T    128.0     64.0     64.0   16.377354\n",
            "29   2048      Triton X@W4^T    128.0    128.0     64.0   17.127275\n",
            "30   2048      Triton X@W4^T    128.0    128.0    128.0   15.792751\n",
            "31   2048      Triton X@W4^T    256.0    128.0    128.0  265.036583\n",
            "32   2048  Pytorch X16@W16^T      NaN      NaN      NaN    3.304100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wiki"
      ],
      "metadata": {
        "id": "QvPXdRxiQHFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'unsloth/Llama-3.2-1B-Instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "fp16_size = model_memory(model_fp16)\n",
        "print(\"FP16 model\")\n",
        "fp16_ppl = perplexity(model_fp16, tokenizer, dataset, samples=256)\n",
        "fp16_speed = compute_speed(model_fp16, tokenizer, dataset, batch_size=16, samples=256)\n",
        "\n",
        "print(f\"Perplexity: {fp16_ppl:.2f}, memory size: {fp16_size:.2f} MB\")\n",
        "\n",
        "del model_fp16\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model_quant = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "model_int4 = quantize_model(model_quant)\n",
        "\n",
        "int4_size = model_memory(model_int4)\n",
        "print(\"INT4 quantized model\")\n",
        "int4_ppl = perplexity(model_int4, tokenizer, dataset, samples=256)\n",
        "int4_speed = compute_speed(model_int4, tokenizer, dataset, batch_size=16, samples=256)\n",
        "\n",
        "print(f\"Perplexity: {int4_ppl:.2f}, memory size: {int4_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1pspa_hQNmm",
        "outputId": "30480ea0-ae1b-47a9-aa1e-457c391657a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:06<00:00, 41.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len: 128 Batch 16: avg time 155.45ms\n",
            "len: 512 Batch 16: avg time 355.38ms\n",
            "len: 2048 Batch 16: avg time 382.21ms\n",
            "Perplexity: 28.03, memory size: 4714.26 MB\n",
            "INT4 quantized model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [01:03<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len: 128 Batch 16: avg time 3172.17ms\n",
            "len: 512 Batch 16: avg time 7442.40ms\n",
            "len: 2048 Batch 16: avg time 7446.27ms\n",
            "Perplexity: 45.23, memory size: 1002.26 MB\n"
          ]
        }
      ]
    }
  ]
}